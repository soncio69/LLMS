{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9525e261-6aaa-4e94-ad34-c6989b264013",
   "metadata": {},
   "source": [
    "# Classification Fine Tuning\n",
    "\n",
    "Questo notebook ha l'obiettivo di effettuare il <b>\"fine tuning\"</b> di un modello pretrained al fine di specializzare il modello stesso rispetto alle capacità già acquisite in fase di pre-traing.\n",
    "\n",
    "A tal fine è necessario utilizzare uno specifico dataset che sia specifico del dominio per il quale si vuole adattare il modello.\n",
    "Attraverso il processo di fine tuning i weigths del modello sono modificati rispetto ai valori ottenuti con la fase di pre-trainig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104be293-b4c8-4de1-9444-ecadab112f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install datasets\n",
    "#!uv pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b3f64-0e53-4e3d-b943-eecb151919a8",
   "metadata": {},
   "source": [
    "A titolo di esempio andremo a fare il fine tuning di un modello per specializzarlo nel compito di sentiment analisys (binary classification) su recensioni di ristoranti (in inglese).\n",
    "\n",
    "A tal fine, come prima cosa, è necessario ottenere un dataset da utilizzare per il fine tuning. Utilizzeremo un dataset scaricato da HF (Yelp Polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa2549c-0c3b-46f2-bd6f-925e52f844d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 560000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 38000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_polarity\") \n",
    "\n",
    "# Inspects the dataset to understand the structure \n",
    "print(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b04d92-99b0-4077-979a-83ef4c6414af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\", 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Accesses the train split \n",
    "train_dataset = dataset['train']  #1\n",
    "\n",
    "# Prints the first example\n",
    "print(train_dataset[0])  #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb466b7-69e1-40b6-b722-373d271ea335",
   "metadata": {},
   "source": [
    "Dal momento che il dataset include recensioni relative a diversi argomenti, è necessario estrarre solo quelle che riguardano ristoranti.<br>\n",
    "Inoltre è necessario ridurre il numero di esempi per facilitare l'attività di SFT (in assenza di hardware adeguato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6977e910-821e-4bad-af51-31ec7d7b7fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Selects the train and test splits \n",
    "train_dataset = dataset[\"train\"] \n",
    "test_dataset = dataset[\"test\"]   \n",
    "\n",
    "# Filters for restaurant-related reviews in the train and test datasets \n",
    "restaurant_train_reviews = train_dataset.filter( \n",
    "    lambda x: \"restaurant\" in x[\"text\"].lower()\n",
    ")\n",
    "\n",
    "restaurant_test_reviews = test_dataset.filter( \n",
    "    lambda x: \"restaurant\" in x[\"text\"].lower()\n",
    ")\n",
    "\n",
    "number_of_reviews = 5000\n",
    "\n",
    "# Shuffles and gets 5,000 rows \n",
    "subset_train_reviews = restaurant_train_reviews.shuffle(\n",
    "    seed = 42).select(range(number_of_reviews))\n",
    "subset_test_reviews = restaurant_test_reviews.shuffle(\n",
    "    seed = 42).select(range(number_of_reviews))\n",
    "\n",
    "# Creates a DatasetDict to return both train and test datasets \n",
    "subset_dataset = {\n",
    "    \"train\": subset_train_reviews,\n",
    "    \"test\": subset_test_reviews\n",
    "}\n",
    "\n",
    "# Displays the structure to match the requested format \n",
    "from datasets import DatasetDict\n",
    "yelp_restaurant_dataset = DatasetDict(subset_dataset)\n",
    "\n",
    "# Prints the dataset structure \n",
    "print(yelp_restaurant_dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c2917-e1d9-494b-ad27-daea0392e673",
   "metadata": {},
   "source": [
    "Visualizzo il primo record del nuovo training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10059da4-c462-415c-8d3b-107bae2c6bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'My girlfriend and I have been wanting to come here for awhile, we finally came & we had the worst experience ever. We asked our server for a few minutes to look over the menu & he never came back. 15 minutes later, someone finally came and took our order. We waited awhile and when they brought our food, they got the whole order wrong. My girlfriend ordered soup and it never came out. Worst service ever. Would not recommend this restaurant to anyone.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_restaurant_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee96673-f8f3-4884-9a0b-a8ec02c6f663",
   "metadata": {},
   "source": [
    "### TOKENIZE DATASET\n",
    "\n",
    "Dopo aver creato il dataset da utilizzare, procedo a scaricare il modello pre-trained (distilbert-base-uncased in questo caso) e il relativo tokenizer e procedo ad effettuare la tokenizzazione del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8beb3c04-9283-4be8-bce5-7cc97a55bf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1333df299ba140d295ae80d3f143652b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Loads a pretrained model and tokenizer (DistilBERT) for sentiment classification \n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Function to tokenize the dataset \n",
    "def tokenize_function(examples):  #2\n",
    "    return tokenizer(examples[\"text\"],\n",
    "                     padding = \"max_length\",\n",
    "                     truncation = True,\n",
    "                     max_length = 512)\n",
    "\n",
    "# Applies the tokenization function to the dataset \n",
    "tokenized_datasets = yelp_restaurant_dataset.map( \n",
    "                         tokenize_function,\n",
    "                         batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e3a94-7152-4b82-8439-fc7444a485f8",
   "metadata": {},
   "source": [
    "Scarico il modello pre-trained su cui effettuare SFT.\n",
    "Utilizzando AutoModelForSequenceClassification viene aggiunta al modello una classification head adatta al task che è necessario eseguire il compito di sequence classification (assegnazione di n labels descrittive a una data sequenza di input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2457b96-8530-4d13-9f99-aeff34f2d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Loads a pretrained model for sequence classification \n",
    "model = AutoModelForSequenceClassification.from_pretrained(  \n",
    "            model_checkpoint, num_labels = 2)\n",
    "\n",
    "# Determines the device \n",
    "if torch.backends.mps.is_available(): \n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Moves the model to the selected device \n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc8199-533d-49c2-8029-4f6df30a6469",
   "metadata": {},
   "source": [
    "## TRAINING\n",
    "\n",
    "Per il training si utilizzano 2 classi di HF:\n",
    "- TrainigArguments : dove vengono settati tutti gli hyperparameter che guidano la fase di training\n",
    "- Trainer : utilizza la classe precedente per fare il training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde07c5c-e3be-4d26-a78a-dd9fad7e7964",
   "metadata": {},
   "source": [
    "# ATTENZIONE!!!\n",
    "\n",
    "NON LANCIARE SU CPU in quanto il kernel va in crash!\n",
    "\n",
    "Da riprovare a lanciare da COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997704b-817c-4aa0-a970-69de6a8be92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c3928-7990-4fdc-be15-c27a0b9f5dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michele/dev/python/LLMs/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/939 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Sets up training arguments \n",
    "training_args = TrainingArguments( \n",
    "    output_dir = \"./results\",  # Directory in which to save results \n",
    "    eval_strategy = \"epoch\",  # Evaluates model after each epoch \n",
    "    save_strategy = \"epoch\",  # Saves the model after each epoch \n",
    "    learning_rate = 2e-5,  # Learning rate \n",
    "    per_device_train_batch_size = 16,  # Batch size for training \n",
    "    per_device_eval_batch_size = 16,  # Batch size for evaluation \n",
    "    num_train_epochs = 3,  # Number of training epochs \n",
    "    weight_decay = 0.01,  # Weight decay for regularization \n",
    "    logging_dir = \"./logs\",  # Directory for logs \n",
    "    logging_steps = 10,  # Logs every 10 steps \n",
    "    save_steps = 500,  # Saves the model every 500 steps \n",
    "    load_best_model_at_end = True,  # Loads the best model at the end of training \n",
    ")\n",
    "\n",
    "# Sets up the Trainer \n",
    "trainer = Trainer( \n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets[\"train\"],\n",
    "    eval_dataset = tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# Fine-tunes the model \n",
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d075d-59ff-49a5-8962-49dc8e714f35",
   "metadata": {},
   "source": [
    "Una volta conclusa la fase di fine tuning è possibile procedere con il salvataggio del nuovo modello e del relativo tokenizer in una specifica cartella. In questo modo sarà possibile successivamente recuperarlo al fine di utilizzarlo per effettuare inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65f389-e227-456c-bfbb-75b7cdd44206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the fine-tuned model and tokenizer \n",
    "model.save_pretrained(\"./results/final_model\")  \n",
    "tokenizer.save_pretrained(\"./results/final_tokenizer\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ceb9e-1f5f-44d5-9d37-3460fcbb1969",
   "metadata": {},
   "source": [
    "E' possibile effettuare la valutazione del model di cui è stato fatto SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ceebfa-03b1-4c2d-89df-5d349b70928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the model \n",
    "eval_results = trainer.evaluate() \n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd5c46-af85-46eb-98eb-ee3f245a2a7a",
   "metadata": {},
   "source": [
    "Una volta concluso il fine tuning e salvato in nuovo modello è possibile utilizzare il modello stesso per effettuare sentiment analysis su nuove recensioni di ristoranti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33fa36-965c-40c8-8de1-a98b1236e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Reloads the model and tokenizer \n",
    "new_model = AutoModelForSequenceClassification.from_pretrained( \n",
    "                \"./results/final_model\")\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(  \n",
    "                \"./results/final_tokenizer\")\n",
    "\n",
    "# Moves the inference to GPU \n",
    "new_model.to(device) \n",
    "\n",
    "sentence = '''\n",
    "I had an amazing experience dining at this restaurant last night.\n",
    "From the moment we walked in, the staff made us feel welcomed and\n",
    "were incredibly attentive. Our server was friendly, knowledgeable,\n",
    "and made great recommendations from the menu.\n",
    "\n",
    "The food was absolutely delicious. I had the grilled salmon, and\n",
    "it was cooked to perfection—tender, flavorful, and served with a\n",
    "lovely citrus glaze that complemented it beautifully. The roasted \n",
    "vegetables on the side were fresh and perfectly seasoned. My\n",
    "partner had the pasta, which was creamy and rich in flavor, with\n",
    "just the right amount of spice.\n",
    "\n",
    "The ambiance was warm and inviting, with cozy lighting and tasteful\n",
    "decor. It was the perfect place to relax and enjoy a nice meal. The \n",
    "dessert, a decadent chocolate lava cake, was the perfect way to end\n",
    "the meal.\n",
    "\n",
    "Overall, this restaurant exceeded my expectations in every way.\n",
    "Excellent food, exceptional service, and a wonderful atmosphere.\n",
    "I'll definitely be back and highly recommend it to anyone looking\n",
    "for a great dining experience.\n",
    "'''\n",
    "\n",
    "# Tokenizes the input sentence \n",
    "inputs = new_tokenizer(sentence, \n",
    "                       return_tensors = \"pt\",\n",
    "                       padding = True,\n",
    "                       truncation = True,\n",
    "                       max_length = 512)\n",
    "\n",
    "# Moves inputs to GPU/MPS \n",
    "inputs = {key: value.to(device) for key, value in inputs.items()} \n",
    "\n",
    "# Puts the model in evaluation mode \n",
    "new_model.eval() \n",
    "\n",
    "# Runs the model to get predictions \n",
    "with torch.no_grad():\n",
    "    outputs = new_model(**inputs) \n",
    "\n",
    "# Gets the logits (raw scores) from the model output \n",
    "logits = outputs.logits \n",
    "# Converts logits to probabilities using Softmax \n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1) \n",
    "# Gets the predicted class (index of the maximum probability) \n",
    "predicted_class = torch.argmax(probabilities, dim=-1).item() \n",
    "\n",
    "# Outputs the predicted sentiment\n",
    "if predicted_class == 1: \n",
    "    print(f\"Sentiment: Positive (Confidence: \\\n",
    "          {probabilities[0][1].item():.2f})\")\n",
    "else:\n",
    "    print(f\"Sentiment: Negative (Confidence: \\\n",
    "           {probabilities[0][0].item():.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

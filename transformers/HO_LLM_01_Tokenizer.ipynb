{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbebed85-7d0d-4f41-98fe-0ca1f552a575",
   "metadata": {},
   "source": [
    "Questo notebook rappresenta un esempio di utilizzo della libreria transformer per interagire con LLM Open Source.<br>\n",
    "Tutti questi esempi sono tratti dal libro \"Hand-on Large Language Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3704d0d0-8cf2-41fb-a406-a78571fc2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9440b763-fdbc-4a0f-aff6-fcedfc97f62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 74.54it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-4-mini-instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=\"True\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained( \"microsoft/Phi-4-mini-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15164f4-e796-42a2-ba30-90e0c731edca",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Il modo pi√π semplice √® quello di utilizzare una pipeline: in questo caso per text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ef858b-bc40-41d1-95e2-aee041cdfffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't chickens play poker in the jungle? Too many pecking problems!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\":     \"user\",\n",
    "     \"content\" : \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "output=generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ed78e-1812-4d0d-b42d-1ffc58020c2a",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Per capire il concetto di tokenizzazione √® possibile scendere di livello rispetto alla pipeline ed utilizzare prima il tokenizer per tokenizzare l'input e poi il model per elaborare l'input tokenizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4f8112-3cfb-46ed-aca4-795403b4db32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>Subject: Sincere Apologies for the Gardening Mishap\n",
      "\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "\n",
      "I hope this message\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=20\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11909526-eead-40f6-a9ef-e4e2f7d56ffe",
   "metadata": {},
   "source": [
    "Avendo spezzato il processo tra tokenizzazione e generazione √® possibile verificare come l'input viene trasformato dal tokenizer in un array di interi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60ab00e-f3e1-4654-a952-8a386749c830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10930,    448,   3719,  39950,   6396,    316,  32145,    395,    290,\n",
       "          62374,  66241,  80785,    403,     13, 115474,   1495,    480,  12570,\n",
       "             13, 200019]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cf841-3940-4213-ab47-84cb44dc0440",
   "metadata": {},
   "source": [
    "Sempre utilizzando il tokenizer √® possibile riconvertire questi interi nei rispettivi token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78acc5d4-fbdf-4ab3-9082-e8cec777b694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write\n",
      " an\n",
      " email\n",
      " apolog\n",
      "izing\n",
      " to\n",
      " Sarah\n",
      " for\n",
      " the\n",
      " tragic\n",
      " gardening\n",
      " mish\n",
      "ap\n",
      ".\n",
      " Explain\n",
      " how\n",
      " it\n",
      " happened\n",
      ".\n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "for id in input_ids[0]:\n",
    "   print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ccf70-0d12-4cd7-89c5-b039654e768a",
   "metadata": {},
   "source": [
    "Anche l'output generato dal modello √® costituito da token.<br>\n",
    "Questi token contengono sia l'input al modello che l'output generato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2595188-44c8-4d9c-8e21-763263eee5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10930,    448,   3719,  39950,   6396,    316,  32145,    395,    290,\n",
       "          62374,  66241,  80785,    403,     13, 115474,   1495,    480,  12570,\n",
       "             13, 200019,  18174,     25,    336,   2768,    512,   6537,  10384,\n",
       "            395,    290, 193145, 147276,    403,   2499,  36210,  32145, 123200,\n",
       "             40,   5498,    495,   3176]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2e25ef7-eb87-4650-b98a-e492f3ac46d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " message\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(3176))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52439e-14c3-4a79-96ab-61dd795a27c1",
   "metadata": {},
   "source": [
    "# Confronto tra tokenizers\n",
    "\n",
    "Test di come un testo venga tokenizzato diversamente a seconda del tokenizer utilizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845f9dce-4d4e-4a7f-9882-336548219c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "English and CAPITALIZATION\n",
    "\n",
    "üéµÈ∏ü\n",
    "show_tokens False None elif == >= else: two tabs:\" \" Three tabs: \"   \"\n",
    "\n",
    "12.0*50=600\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd640b8-2601-4a82-9151-b045f851e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list = [\n",
    "    '102;194;165', '252;141;98', '141;160;203', \n",
    "    '231;138;195', '166;216;84', '255;217;47'\n",
    "]\n",
    "\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' + \n",
    "            tokenizer.decode(t) + \n",
    "            '\\x1b[0m', \n",
    "            end=' '\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab67cd-2293-4f88-a9a2-8f57f1c927e4",
   "metadata": {},
   "source": [
    "Come primo esempio vediamo <b>WordPiece</b> (usato per Bert)\n",
    "\n",
    "Da notare cone tutte le lettere siano minuscole, di come non vengano considerate le newline e l'esistenza di unknow tokens[UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14d9701-baf7-4bbb-aed8-186a24844ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98menglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mcapital\u001b[0m \u001b[0;30;48;2;166;216;84m##ization\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtoken\u001b[0m \u001b[0;30;48;2;166;216;84m##s\u001b[0m \u001b[0;30;48;2;255;217;47mfalse\u001b[0m \u001b[0;30;48;2;102;194;165mnone\u001b[0m \u001b[0;30;48;2;252;141;98meli\u001b[0m \u001b[0;30;48;2;141;160;203m##f\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m>\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98melse\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195mtwo\u001b[0m \u001b[0;30;48;2;166;216;84mtab\u001b[0m \u001b[0;30;48;2;255;217;47m##s\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195mthree\u001b[0m \u001b[0;30;48;2;166;216;84mtab\u001b[0m \u001b[0;30;48;2;255;217;47m##s\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m12\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m*\u001b[0m \u001b[0;30;48;2;252;141;98m50\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m600\u001b[0m \u001b[0;30;48;2;166;216;84m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247686a-d7cf-4c8a-b747-146ac0bfe31d",
   "metadata": {},
   "source": [
    "Vediamo il tokenizer <b>BPE</b> utilizzato da GPT-2\n",
    "\n",
    "Da notare il carattere newline, la presenza di maiuscole e la tokenizzazione dei caratteri simbolici\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "906ba46e-b1dd-4cd9-8542-1bc71cffda5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;102;194;165m\n",
      "\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
      "\u001b[0m \u001b[0;30;48;2;141;160;203mEnglish\u001b[0m \u001b[0;30;48;2;231;138;195m and\u001b[0m \u001b[0;30;48;2;166;216;84m CAP\u001b[0m \u001b[0;30;48;2;255;217;47mITAL\u001b[0m \u001b[0;30;48;2;102;194;165mIZ\u001b[0m \u001b[0;30;48;2;252;141;98mATION\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
      "\u001b[0m \u001b[0;30;48;2;231;138;195m\n",
      "\u001b[0m \u001b[0;30;48;2;166;216;84mÔøΩ\u001b[0m \u001b[0;30;48;2;255;217;47mÔøΩ\u001b[0m \u001b[0;30;48;2;102;194;165mÔøΩ\u001b[0m \u001b[0;30;48;2;252;141;98mÔøΩ\u001b[0m \u001b[0;30;48;2;141;160;203mÔøΩ\u001b[0m \u001b[0;30;48;2;231;138;195mÔøΩ\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
      "\u001b[0m \u001b[0;30;48;2;255;217;47mshow\u001b[0m \u001b[0;30;48;2;102;194;165m_\u001b[0m \u001b[0;30;48;2;252;141;98mt\u001b[0m \u001b[0;30;48;2;141;160;203mok\u001b[0m \u001b[0;30;48;2;231;138;195mens\u001b[0m \u001b[0;30;48;2;166;216;84m False\u001b[0m \u001b[0;30;48;2;255;217;47m None\u001b[0m \u001b[0;30;48;2;102;194;165m el\u001b[0m \u001b[0;30;48;2;252;141;98mif\u001b[0m \u001b[0;30;48;2;141;160;203m ==\u001b[0m \u001b[0;30;48;2;231;138;195m >=\u001b[0m \u001b[0;30;48;2;166;216;84m else\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m two\u001b[0m \u001b[0;30;48;2;252;141;98m tabs\u001b[0m \u001b[0;30;48;2;141;160;203m:\"\u001b[0m \u001b[0;30;48;2;231;138;195m \"\u001b[0m \u001b[0;30;48;2;166;216;84m Three\u001b[0m \u001b[0;30;48;2;255;217;47m tabs\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m \"\u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m \"\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
      "\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
      "\u001b[0m \u001b[0;30;48;2;252;141;98m12\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m*\u001b[0m \u001b[0;30;48;2;255;217;47m50\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m600\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
      "\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfc868-012b-4184-b1d0-2f9a6efd3db1",
   "metadata": {},
   "source": [
    "Vediamo ora Flan-T5 che utilizza <b>SentencePiece</b>\n",
    "\n",
    "Anche in questo caso non viene gestito newline e abbiamo unknown per i token non Inglesi e Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd99131d-7a8e-44ce-926d-43d2e95a0104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;102;194;165mEnglish\u001b[0m \u001b[0;30;48;2;252;141;98mand\u001b[0m \u001b[0;30;48;2;141;160;203mCA\u001b[0m \u001b[0;30;48;2;231;138;195mPI\u001b[0m \u001b[0;30;48;2;166;216;84mTAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZ\u001b[0m \u001b[0;30;48;2;102;194;165mATION\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203m<unk>\u001b[0m \u001b[0;30;48;2;231;138;195mshow\u001b[0m \u001b[0;30;48;2;166;216;84m_\u001b[0m \u001b[0;30;48;2;255;217;47mto\u001b[0m \u001b[0;30;48;2;102;194;165mken\u001b[0m \u001b[0;30;48;2;252;141;98ms\u001b[0m \u001b[0;30;48;2;141;160;203mFal\u001b[0m \u001b[0;30;48;2;231;138;195ms\u001b[0m \u001b[0;30;48;2;166;216;84me\u001b[0m \u001b[0;30;48;2;255;217;47mNone\u001b[0m \u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98me\u001b[0m \u001b[0;30;48;2;141;160;203ml\u001b[0m \u001b[0;30;48;2;231;138;195mif\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165m>\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203melse\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84mtwo\u001b[0m \u001b[0;30;48;2;255;217;47mtab\u001b[0m \u001b[0;30;48;2;102;194;165ms\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84mThree\u001b[0m \u001b[0;30;48;2;255;217;47mtab\u001b[0m \u001b[0;30;48;2;102;194;165ms\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m12.\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m*\u001b[0m \u001b[0;30;48;2;252;141;98m50\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m600\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47m</s>\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206fd265-ab8b-4183-9b14-1a402dc540c0",
   "metadata": {},
   "source": [
    "Infine vediamo il tokenization schema di phi4 che utilizza anche lui BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ebebc72-d5cb-4a6e-9b2a-1f584bf22c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;102;194;165m\n",
      "\n",
      "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
      "\n",
      "\u001b[0m \u001b[0;30;48;2;102;194;165mÔøΩ\u001b[0m \u001b[0;30;48;2;252;141;98mÔøΩ\u001b[0m \u001b[0;30;48;2;141;160;203mÈ∏ü\u001b[0m \u001b[0;30;48;2;231;138;195m\n",
      "\u001b[0m \u001b[0;30;48;2;166;216;84mshow\u001b[0m \u001b[0;30;48;2;255;217;47m_tokens\u001b[0m \u001b[0;30;48;2;102;194;165m False\u001b[0m \u001b[0;30;48;2;252;141;98m None\u001b[0m \u001b[0;30;48;2;141;160;203m elif\u001b[0m \u001b[0;30;48;2;231;138;195m ==\u001b[0m \u001b[0;30;48;2;166;216;84m >=\u001b[0m \u001b[0;30;48;2;255;217;47m else\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m two\u001b[0m \u001b[0;30;48;2;141;160;203m tabs\u001b[0m \u001b[0;30;48;2;231;138;195m:\"\u001b[0m \u001b[0;30;48;2;166;216;84m \"\u001b[0m \u001b[0;30;48;2;255;217;47m Three\u001b[0m \u001b[0;30;48;2;102;194;165m tabs\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m \"\u001b[0m \u001b[0;30;48;2;231;138;195m  \u001b[0m \u001b[0;30;48;2;166;216;84m \"\n",
      "\n",
      "\u001b[0m \u001b[0;30;48;2;255;217;47m12\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m0\u001b[0m \u001b[0;30;48;2;141;160;203m*\u001b[0m \u001b[0;30;48;2;231;138;195m50\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m600\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
      "\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"microsoft/Phi-4-mini-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a1b11-2b8c-45ba-a3e5-c679f8170540",
   "metadata": {},
   "source": [
    "# EMBEDDINGS\n",
    "Dopo aver suddiviso il testo iniziale in tokens, √® necessario convertire questi tokens in una rappresentazione numerica in modo che poi questa rappresentazione possa essere passata al modello per il training.\n",
    "Il modello contiene l'embedding matrix che mappa i token creati col tokenizer ai rispettivi embeddings. Questo √® il motivo per cui ogni modello deve utilizzare il rispettivo tokenizer.<br>\n",
    "All'inizio del training questa matrice contiene valori random che vengono aggiornati nel corso del training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3b880-0eeb-4597-acd9-46b1105566ca",
   "metadata": {},
   "source": [
    "Verifico come utilizzare un modello transformer per generare dei contextualized token embeddings, cio√® degli embeddings in cui il valore dipende dal contesto della frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5720f92e-b668-470b-a5a5-ca3e75275105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "# Load a language model\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer('Hello world', return_tensors='pt')\n",
    "\n",
    "# Process the tokens\n",
    "output = model(**tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa6423d9-f16f-4281-9061-640352b18a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 384])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e994735-7811-42c2-b65c-8100ba00554f",
   "metadata": {},
   "source": [
    "In questo caso la frase Hello World √® stata rappresentata da 4 tokens e ciascun token √® stato rappresentato da un vettore di 384 VALORI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af90254-02b8-4b65-9264-e5a0f070fef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "Hello\n",
      " world\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for token in tokens['input_ids'][0]:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260986f-e90a-40d5-a068-d681c8d81072",
   "metadata": {},
   "source": [
    "L'output del language model √® quindi il seguente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf8ac227-46cd-450c-bbb5-fdf163b850db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.4816,  0.0861, -0.1819,  ..., -0.0612, -0.3911,  0.3017],\n",
       "         [ 0.1898,  0.3208, -0.2315,  ...,  0.3714,  0.2478,  0.8048],\n",
       "         [ 0.2071,  0.5036, -0.0485,  ...,  1.2175, -0.2292,  0.8582],\n",
       "         [-3.4278,  0.0645, -0.1427,  ...,  0.0658, -0.4367,  0.3834]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199469ac-28a1-46e7-b19b-c804a2ac7475",
   "metadata": {},
   "source": [
    "# TEXT Embeddings\n",
    "\n",
    "Dopo aver visto come un model pu√≤ produrre token embeddings, √® necessario capire come creare embeddings che rappresentino testi come frasi, paragrafi o interi documenti.<br>\n",
    "Anche per questo scopo vengono utilizzati modelli di cui viene fatto un training per questo scopo specifico.<br>\n",
    "\n",
    "Per utilizzare questi pretrained text embedding model si utilizza il package <b>SENTENCE-TRANSFORMERS</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4ba1561-dbed-4fe0-8001-cb7616cb9593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Convert text to text embeddings\n",
    "vector = model.encode(\"Best movie ever!\")\n",
    "\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c3500-e057-4446-99f7-00afbb58dc2a",
   "metadata": {},
   "source": [
    "Si pu√≤ quindi vedere come il testo in input sia stato convertito in un vettore di 768 interi che ne identificano il significato semantico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc567d-92d7-49ad-ab61-2e4b22b8c977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
